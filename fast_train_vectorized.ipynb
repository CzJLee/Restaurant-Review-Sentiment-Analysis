{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdAAAg_J5PxA"
      },
      "source": [
        "# Fast NGram TPU Training\n",
        "\n",
        "After reading few a few articles, I would like to try to implement a few speedups for TPU training\n",
        "\n",
        "- https://www.tensorflow.org/guide/data\n",
        "- https://www.tensorflow.org/guide/data_performance\n",
        "- https://www.tensorflow.org/guide/tpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LMGcv9V25PxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86aQbViP9NjE",
        "outputId": "0eeb6870-4197-42ec-e4cf-9a6d735b985d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.7.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Your runtime has 37.8 gigabytes of available RAM\n",
            "You are using a high-RAM runtime!\n",
            "Running on TPU  ['10.26.37.186:8470']\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.26.37.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.26.37.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "# Check if Google Colab Instance for Setup\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Get correct path if on Google Colab\n",
        "try:\n",
        "\tfrom google.colab import drive\n",
        "\tdrive.mount(\"/content/drive\")\n",
        "\treviews_dataset_path = \"drive/MyDrive/Colab Notebooks/reviews.json\"\n",
        "\n",
        "\t# Get RAM Info\n",
        "\tfrom psutil import virtual_memory\n",
        "\tram_gb = virtual_memory().total / 1e9\n",
        "\tprint('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "\tif ram_gb < 20:\n",
        "\t\tprint('Not using a high-RAM runtime')\n",
        "\telse:\n",
        "\t\tprint('You are using a high-RAM runtime!')\n",
        "\n",
        "\ttry:\n",
        "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "\t\tprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
        "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\t\ttpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "\t\tusing_tpu = True\n",
        "\texcept ValueError:\n",
        "\t\tprint(\"Note: Not connected to a TPU runtime.\")\n",
        "\t\tusing_tpu = False\n",
        "except ModuleNotFoundError:\n",
        "\treviews_dataset_path = \"yelp_dataset/reviews.json\"\n",
        "\tusing_tpu = False\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NsuxbHGn5PxH"
      },
      "outputs": [],
      "source": [
        "# Read dataset into memory\n",
        "review_df = pd.read_json(reviews_dataset_path, orient=\"records\", lines=True)\n",
        "\n",
        "# Shuffle Review df\n",
        "review_df = shuffle(review_df, random_state=0)\n",
        "\n",
        "# Slice into Train, Val, Test at 60:20:20\n",
        "n = len(review_df)\n",
        "df_train = review_df.iloc[: int(n*0.6)]\n",
        "df_val = review_df.iloc[int(n*0.6) : int(n*0.8)]\n",
        "df_test = review_df.iloc[int(n*0.8) :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PsB0709maBfe"
      },
      "outputs": [],
      "source": [
        "# Convert Pandas DF to TF Dataset\n",
        "def convert_text_df_to_dataset(df, input_col=\"text\", target_col=\"stars\"):\n",
        "\ttext_input = tf.convert_to_tensor(df[input_col], dtype=tf.string)\n",
        "\ttarget = tf.convert_to_tensor(df[target_col], dtype=tf.int8)\n",
        "\tdataset = tf.data.Dataset.from_tensor_slices((text_input, target))\n",
        "\treturn dataset\n",
        "\n",
        "train_dataset = convert_text_df_to_dataset(df_train)\n",
        "val_dataset = convert_text_df_to_dataset(df_val)\n",
        "test_dataset = convert_text_df_to_dataset(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvxNxzL29NjL"
      },
      "source": [
        "## Bigram IDF Vectorization - Categorical Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vKxQrno59NjM"
      },
      "outputs": [],
      "source": [
        "# Create TextVectorization\n",
        "max_tokens = 30000\n",
        "# Use IDF vectorization to match train_vectorized.ipynb to compare times\n",
        "text_vectorization_idf = TextVectorization(max_tokens=max_tokens, ngrams=2, output_mode=\"tf_idf\")\n",
        "\n",
        "# Train Vectorizer on train text\n",
        "text_vectorization_idf.adapt(df_train[\"text\"], batch_size=2**16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6o8xAyIpaBff"
      },
      "outputs": [],
      "source": [
        "# Optimize Dataset feedthrogh\n",
        "if using_tpu:\n",
        "\t# TPU's really like big batches I guess. \n",
        "\t# By increasing the batch size by a factor of 128, I am seeing about a 4x speedup. \n",
        "\tbatch_size = 16 * 128 * tpu_strategy.num_replicas_in_sync\n",
        "else:\n",
        "\tbatch_size = 128\n",
        "\n",
        "num_train_epochs = len(df_train) // batch_size\n",
        "\n",
        "# Vectorize Datasets\n",
        "train_dataset_vectorized = train_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)\n",
        "val_dataset_vectorized = val_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)\n",
        "test_dataset_vectorized = test_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)\n",
        "\n",
        "# Repeat, then batch\n",
        "# https://www.tensorflow.org/guide/data#processing_multiple_epochs\n",
        "\n",
        "# Use drop_remainder to get shape prop\n",
        "# https://www.tensorflow.org/guide/data#simple_batching\n",
        "\n",
        "# Use num_parallel_calls on batch\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
        "\n",
        "# Only shuffle and repeat the dataset in training. The advantage of having an infinite dataset for training is to avoid the potential last partial batch in each epoch, so that you don't need to think about scaling the gradients based on the actual batch size.\n",
        "# https://www.tensorflow.org/guide/tpu#load_the_dataset\n",
        "\n",
        "# After testing different permutations, this seems to be the best combo. \n",
        "train_dataset_vectorized = train_dataset_vectorized.repeat().prefetch(num_train_epochs)\n",
        "val_dataset_vectorized = val_dataset_vectorized.prefetch(AUTO)\n",
        "test_dataset_vectorized = test_dataset_vectorized.prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NQmc0GzaBfg"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l-ycaHD89NjS"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def create_model_categorical(max_tokens, model_name):\n",
        "\tinputs = keras.Input(shape=(max_tokens,))\n",
        "\tx = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\tx = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\toutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "\tmodel = keras.Model(inputs, outputs, name=model_name)\n",
        "\n",
        "\t# To reduce Python overhead and maximize the performance of your TPU, pass in the argument steps_per_execution to Model.compile.\n",
        "\t# https://www.tensorflow.org/guide/tpu#train_the_model_using_keras_high-level_apis\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=\"rmsprop\", \n",
        "\t\tloss=\"sparse_categorical_crossentropy\", \n",
        "\t\tmetrics=[\"sparse_categorical_accuracy\"]\n",
        "\t\t)\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMnCObF9NjS",
        "outputId": "72e6bd9a-b4ac-4565-b090-a6348ce0f342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 39s 801ms/step - loss: 1.2233 - sparse_categorical_accuracy: 0.5643 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7243\n",
            "Epoch 2/20\n",
            "37/37 [==============================] - 22s 596ms/step - loss: 0.8468 - sparse_categorical_accuracy: 0.6914 - val_loss: 0.6995 - val_sparse_categorical_accuracy: 0.7435\n",
            "Epoch 3/20\n",
            "37/37 [==============================] - 24s 658ms/step - loss: 0.7423 - sparse_categorical_accuracy: 0.7199 - val_loss: 0.6530 - val_sparse_categorical_accuracy: 0.7517\n",
            "Epoch 4/20\n",
            "37/37 [==============================] - 24s 669ms/step - loss: 0.6833 - sparse_categorical_accuracy: 0.7368 - val_loss: 0.6393 - val_sparse_categorical_accuracy: 0.7523\n",
            "Epoch 5/20\n",
            "37/37 [==============================] - 24s 647ms/step - loss: 0.6472 - sparse_categorical_accuracy: 0.7482 - val_loss: 0.6291 - val_sparse_categorical_accuracy: 0.7504\n",
            "Epoch 6/20\n",
            "37/37 [==============================] - 24s 661ms/step - loss: 0.6165 - sparse_categorical_accuracy: 0.7583 - val_loss: 0.6519 - val_sparse_categorical_accuracy: 0.7430\n",
            "Epoch 7/20\n",
            "37/37 [==============================] - 24s 668ms/step - loss: 0.5943 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.6219 - val_sparse_categorical_accuracy: 0.7533\n",
            "Epoch 8/20\n",
            "37/37 [==============================] - 24s 659ms/step - loss: 0.5703 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.6325 - val_sparse_categorical_accuracy: 0.7531\n",
            "Epoch 9/20\n",
            "37/37 [==============================] - 24s 646ms/step - loss: 0.5563 - sparse_categorical_accuracy: 0.7776 - val_loss: 0.6674 - val_sparse_categorical_accuracy: 0.7418\n",
            "Epoch 10/20\n",
            "37/37 [==============================] - 24s 663ms/step - loss: 0.5391 - sparse_categorical_accuracy: 0.7843 - val_loss: 0.6530 - val_sparse_categorical_accuracy: 0.7520\n",
            "Epoch 00010: early stopping\n",
            "Model vectorized_categorical_idf_fast_train with MAE 0.298 and MSE 0.451\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = \"vectorized_categorical_idf_fast_train\"\n",
        "\n",
        "# Creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "if using_tpu:\n",
        "\twith tpu_strategy.scope():\n",
        "\t\tmodel = create_model_categorical(max_tokens, model_name)\n",
        "else:\n",
        "\tmodel = create_model_categorical(max_tokens, model_name)\n",
        "\n",
        "# Create callback to save model with a given name\n",
        "model_path = f\"models/{model_name}.keras\"\n",
        "callbacks = [\n",
        "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
        "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=False)\n",
        "]\n",
        "\n",
        "# Train Model\n",
        "# Previous throughput was about 29s per training epoch. Let's see if we can beat that with the optimization changes. \n",
        "model.fit(train_dataset_vectorized, \n",
        "\tvalidation_data=val_dataset_vectorized, \n",
        "\tsteps_per_epoch=num_train_epochs, \n",
        "\tepochs=20, \n",
        "\tcallbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate Model after training\n",
        "model = keras.models.load_model(model_path)\n",
        "predictions = model.predict(test_dataset_vectorized)\n",
        "predictions = np.argmax(predictions, axis = -1)\n",
        "true_labels = np.concatenate([y for _, y in test_dataset_vectorized], axis=0)\n",
        "mae = mean_absolute_error(true_labels, predictions)\n",
        "mse = mean_squared_error(true_labels, predictions)\n",
        "\n",
        "\n",
        "# Output Model Metrics\n",
        "metrics_text = f\"Model {model_name} with MAE {mae:.3f} and MSE {mse:.3f}\\n\"\n",
        "print(metrics_text)\n",
        "with open(\"model_metrics.txt\", \"a\") as f:\n",
        "\tf.write(metrics_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mJX5SqEe9NjT",
        "outputId": "521ff22c-c530-44d3-fd26-ffb0f2faa7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: models/ (stored 0%)\n",
            "updating: models/vectorized_categorical_idf_fast_train.keras (deflated 9%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8795258a-1913-4e76-a0ec-b9054d0dd9df\", \"models.zip\", 7056469)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_86e7d6c3-07f0-4d4e-9c76-b2e1d1a62c24\", \"model_metrics.txt\", 73)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Zip Models\n",
        "!zip -r \"models.zip\" \"models\"\n",
        "\n",
        "try: \n",
        "\tfrom google.colab import files\n",
        "\tfiles.download(\"models.zip\")\n",
        "\tfiles.download(\"model_metrics.txt\")\n",
        "except:\n",
        "\tpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J_UFmpBQe7oG"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fast_train_vectorized.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}