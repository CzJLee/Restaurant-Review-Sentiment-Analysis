{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdAAAg_J5PxA"
      },
      "source": [
        "# Fast NGram TPU Training\n",
        "\n",
        "After reading few a few articles, I would like to try to implement a few speedups for TPU training\n",
        "\n",
        "- https://www.tensorflow.org/guide/data\n",
        "- https://www.tensorflow.org/guide/data_performance\n",
        "- https://www.tensorflow.org/guide/tpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMGcv9V25PxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86aQbViP9NjE",
        "outputId": "499a0b41-e315-4242-cc34-c6ccaa4ab31e"
      },
      "outputs": [],
      "source": [
        "# Check if Google Colab Instance for Setup\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Get correct path if on Google Colab\n",
        "try:\n",
        "\tfrom google.colab import drive\n",
        "\tdrive.mount(\"/content/drive\")\n",
        "\treviews_dataset_path = \"drive/MyDrive/Colab Notebooks/reviews.json\"\n",
        "\n",
        "\t# Get RAM Info\n",
        "\tfrom psutil import virtual_memory\n",
        "\tram_gb = virtual_memory().total / 1e9\n",
        "\tprint('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "\tif ram_gb < 20:\n",
        "\t\tprint('Not using a high-RAM runtime')\n",
        "\telse:\n",
        "\t\tprint('You are using a high-RAM runtime!')\n",
        "\n",
        "\ttry:\n",
        "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "\t\tprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
        "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\t\ttpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "\t\tusing_tpu = True\n",
        "\texcept ValueError:\n",
        "\t\tprint(\"Note: Not connected to a TPU runtime.\")\n",
        "\t\tusing_tpu = False\n",
        "except ModuleNotFoundError:\n",
        "\treviews_dataset_path = \"yelp_dataset/reviews.json\"\n",
        "\tusing_tpu = False\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsuxbHGn5PxH"
      },
      "outputs": [],
      "source": [
        "# Read dataset into memory\n",
        "review_df = pd.read_json(reviews_dataset_path, orient=\"records\", lines=True)\n",
        "\n",
        "# Shuffle Review df\n",
        "review_df = shuffle(review_df, random_state=0)\n",
        "\n",
        "# Slice into Train, Val, Test at 60:20:20\n",
        "n = len(review_df)\n",
        "df_train = review_df.iloc[: int(n*0.6)]\n",
        "df_val = review_df.iloc[int(n*0.6) : int(n*0.8)]\n",
        "df_test = review_df.iloc[int(n*0.8) :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Pandas DF to TF Dataset\n",
        "\n",
        "\n",
        "def convert_text_df_to_dataset(df, input_col=\"text\", target_col=\"stars\"):\n",
        "\ttext_input = tf.convert_to_tensor(df[input_col], dtype=tf.string)\n",
        "\ttarget = tf.convert_to_tensor(df[target_col], dtype=tf.int8)\n",
        "\tdataset = tf.data.Dataset.from_tensor_slices((text_input, target))\n",
        "\treturn dataset\n",
        "\n",
        "train_dataset = convert_text_df_to_dataset(df_train)\n",
        "val_dataset = convert_text_df_to_dataset(df_val)\n",
        "test_dataset = convert_text_df_to_dataset(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvxNxzL29NjL"
      },
      "source": [
        "## Bigram IDF Vectorization - Categorical Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKxQrno59NjM"
      },
      "outputs": [],
      "source": [
        "# Create TextVectorization\n",
        "max_tokens = 30000\n",
        "# Use IDF vectorization to match train_vectorized.ipynb to compare times\n",
        "text_vectorization_idf = TextVectorization(max_tokens=max_tokens, ngrams=2, output_mode=\"tf_idf\")\n",
        "\n",
        "# Train Vectorizer on train text\n",
        "text_vectorization_idf.adapt(df_train[\"text\"])\n",
        "\n",
        "# Vectorize Datasets\n",
        "train_dataset_vectorized = train_dataset.map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)\n",
        "val_dataset_vectorized = val_dataset.map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)\n",
        "test_dataset_vectorized = test_dataset.map(lambda x, y: (text_vectorization_idf(x), y-1), num_parallel_calls=AUTO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize Dataset feedthrogh\n",
        "if using_tpu:\n",
        "\t# TPU's really like big batches I guess. \n",
        "\t# By increasing the batch size by a factor of 128, I am seeing about a 4x speedup. \n",
        "\tbatch_size = 16 * 128 * tpu_strategy.num_replicas_in_sync\n",
        "else:\n",
        "\tbatch_size = 128\n",
        "\n",
        "num_train_epochs = len(df_train) // batch_size\n",
        "\n",
        "# Repeat, then batch\n",
        "# https://www.tensorflow.org/guide/data#processing_multiple_epochs\n",
        "\n",
        "# Use drop_remainder to get shape prop\n",
        "# https://www.tensorflow.org/guide/data#simple_batching\n",
        "\n",
        "# Use num_parallel_calls on batch\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
        "\n",
        "# Only shuffle and repeat the dataset in training. The advantage of having an infinite dataset for training is to avoid the potential last partial batch in each epoch, so that you don't need to think about scaling the gradients based on the actual batch size.\n",
        "# https://www.tensorflow.org/guide/tpu#load_the_dataset\n",
        "\n",
        "train_dataset_vectorized = train_dataset_vectorized.repeat().batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO).prefetch(AUTO)\n",
        "val_dataset_vectorized = val_dataset_vectorized.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO).prefetch(AUTO)\n",
        "test_dataset_vectorized = test_dataset_vectorized.batch(batch_size, drop_remainder=False, num_parallel_calls=AUTO).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-ycaHD89NjS"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def create_model_categorical(max_tokens, model_name):\n",
        "\tinputs = keras.Input(shape=(max_tokens,))\n",
        "\tx = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\tx = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\toutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "\tmodel = keras.Model(inputs, outputs, name=model_name)\n",
        "\n",
        "\t# To reduce Python overhead and maximize the performance of your TPU, pass in the argument steps_per_execution to Model.compile.\n",
        "\t# https://www.tensorflow.org/guide/tpu#train_the_model_using_keras_high-level_apis\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=\"rmsprop\", \n",
        "\t\tloss=\"sparse_categorical_crossentropy\", \n",
        "\t\tmetrics=[\"sparse_categorical_accuracy\"],\n",
        "\t\tsteps_per_execution=32\n",
        "\t\t)\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMnCObF9NjS",
        "outputId": "1c01ad38-2bec-4e23-857b-b231fff38b4d"
      },
      "outputs": [],
      "source": [
        "model_name = \"vectorized_categorical_idf_fast_train\"\n",
        "\n",
        "# Creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "if using_tpu:\n",
        "\twith tpu_strategy.scope():\n",
        "\t\tmodel = create_model_categorical(max_tokens, model_name)\n",
        "else:\n",
        "\tmodel = create_model_categorical(max_tokens, model_name)\n",
        "\n",
        "# Create callback to save model with a given name\n",
        "model_path = f\"models/{model_name}.keras\"\n",
        "callbacks = [\n",
        "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
        "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=False)\n",
        "]\n",
        "\n",
        "# Train Model\n",
        "# Previous throughput was about 29s per training epoch. Let's see if we can beat that with the optimization changes. \n",
        "model.fit(train_dataset_vectorized, \n",
        "\tvalidation_data=val_dataset_vectorized, \n",
        "\tsteps_per_epoch=num_train_epochs, \n",
        "\tepochs=20, \n",
        "\tcallbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate Model after training\n",
        "model = keras.models.load_model(model_path)\n",
        "predictions = model.predict(test_dataset_vectorized)\n",
        "predictions = np.argmax(predictions, axis = -1)\n",
        "true_labels = np.concatenate([y for _, y in test_dataset_vectorized], axis=0)\n",
        "mae = mean_absolute_error(true_labels, predictions)\n",
        "mse = mean_squared_error(true_labels, predictions)\n",
        "\n",
        "\n",
        "# Output Model Metrics\n",
        "metrics_text = f\"Model {model_name} with MAE {mae:.3f} and MSE {mse:.3f}\\n\"\n",
        "print(metrics_text)\n",
        "with open(\"model_metrics.txt\", \"a\") as f:\n",
        "\tf.write(metrics_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mJX5SqEe9NjT",
        "outputId": "0856f953-fb20-485c-8273-378c8c1eb0b9"
      },
      "outputs": [],
      "source": [
        "# Zip Models\n",
        "!zip -r \"models.zip\" \"models\"\n",
        "\n",
        "try: \n",
        "\tfrom google.colab import files\n",
        "\tfiles.download(\"models.zip\")\n",
        "\tfiles.download(\"model_metrics.txt\")\n",
        "except:\n",
        "\tpass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_vectorized.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
