{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Predicting Rating Based On Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct path if on Google Colab\n",
    "try:\n",
    "\tfrom google.colab import drive\n",
    "\tdrive.mount(\"/content/drive\")\n",
    "\treviews_dataset_path = \"drive/MyDrive/Colab Notebooks/reviews.json\"\n",
    "\n",
    "\t# Get RAM Info\n",
    "\tfrom psutil import virtual_memory\n",
    "\tram_gb = virtual_memory().total / 1e9\n",
    "\tprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "\tif ram_gb < 20:\n",
    "\t\tprint('Not using a high-RAM runtime')\n",
    "\telse:\n",
    "\t\tprint('You are using a high-RAM runtime!')\n",
    "except ModuleNotFoundError:\n",
    "\treviews_dataset_path = \"yelp_dataset/reviews.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset into memory\n",
    "review_df = pd.read_json(reviews_dataset_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Review df\n",
    "review_df = shuffle(review_df, random_state=0)\n",
    "\n",
    "# Slice into Train, Val, Test at 60:20:20\n",
    "n = len(review_df)\n",
    "df_train = review_df.iloc[: int(n*0.6)]\n",
    "df_val = review_df.iloc[int(n*0.6) : int(n*0.8)]\n",
    "df_test = review_df.iloc[int(n*0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DF to TF Dataset\n",
    "\n",
    "def convert_text_df_to_dataset(df, input_col=\"text\", target_col=\"stars\"):\n",
    "\ttext_input = tf.convert_to_tensor(df[input_col], dtype=tf.string)\n",
    "\ttarget = tf.convert_to_tensor(df[target_col], dtype=tf.int8)\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices((text_input, target))\n",
    "\tdataset = dataset.batch(32)\n",
    "\treturn dataset\n",
    "\n",
    "train_dataset = convert_text_df_to_dataset(df_train)\n",
    "val_dataset = convert_text_df_to_dataset(df_val)\n",
    "test_dataset = convert_text_df_to_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TextVectorization\n",
    "max_tokens = 30000\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens, output_mode=\"multi_hot\")\n",
    "\n",
    "# Train Vectorizer on train text\n",
    "text_vectorization.adapt(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Datasets\n",
    "train_dataset = train_dataset.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "val_dataset = val_dataset.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "test_dataset = test_dataset.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                960032    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 960,577\n",
      "Trainable params: 960,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "inputs = keras.Input(shape=(max_tokens,))\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "outputs = keras.layers.ReLU(max_value=5, threshold=0)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mean_absolute_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "model_path = \"models/text_vectorized.keras\"\n",
    "callbacks = [\n",
    "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
    "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=5, verbose=1, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 292s 292s/step - loss: 3.7323 - mean_squared_error: 16.3313 - val_loss: 3.3458 - val_mean_squared_error: 13.8037\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 174s 174s/step - loss: 3.3789 - mean_squared_error: 14.0179 - val_loss: 2.9831 - val_mean_squared_error: 11.4178\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 168s 168s/step - loss: 3.0474 - mean_squared_error: 11.8341 - val_loss: 2.7735 - val_mean_squared_error: 10.0211\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 161s 161s/step - loss: 2.8546 - mean_squared_error: 10.5689 - val_loss: 2.6209 - val_mean_squared_error: 9.0347\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 176s 176s/step - loss: 2.7107 - mean_squared_error: 9.6618 - val_loss: 2.4971 - val_mean_squared_error: 8.2653\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 160s 160s/step - loss: 2.5908 - mean_squared_error: 8.9241 - val_loss: 2.3924 - val_mean_squared_error: 7.6407\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 171s 171s/step - loss: 2.4904 - mean_squared_error: 8.3369 - val_loss: 2.3035 - val_mean_squared_error: 7.1327\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 167s 167s/step - loss: 2.4018 - mean_squared_error: 7.8397 - val_loss: 2.2260 - val_mean_squared_error: 6.7091\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 158s 158s/step - loss: 2.3273 - mean_squared_error: 7.4383 - val_loss: 2.1575 - val_mean_squared_error: 6.3511\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 162s 162s/step - loss: 2.2581 - mean_squared_error: 7.0748 - val_loss: 2.0960 - val_mean_squared_error: 6.0434\n",
      "1/1 [==============================] - 152s 152s/step - loss: 2.0960 - mean_squared_error: 6.0484\n",
      "Test MSE: 6.048\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset.cache(), validation_data=val_dataset.cache(), epochs=50, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 48s 48s/step - loss: 2.0880 - mean_squared_error: 6.0125\n",
      "Test MSE: 6.013\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(model_path)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
