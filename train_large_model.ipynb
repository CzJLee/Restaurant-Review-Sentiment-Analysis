{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Large Vectorized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import time\n",
    "import tarfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Colab TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Google Colab Instance for Setup\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# Get correct path if on Google Colab\n",
    "try:\n",
    "\tfrom google.colab import drive\n",
    "\tdrive.mount(\"/content/drive\")\n",
    "\n",
    "\t# Get RAM Info\n",
    "\tfrom psutil import virtual_memory\n",
    "\tram_gb = virtual_memory().total / 1e9\n",
    "\tprint('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
    "\n",
    "\tif ram_gb < 20:\n",
    "\t\tprint('Not using a high-RAM runtime')\n",
    "\telse:\n",
    "\t\tprint('You are using a high-RAM runtime!')\n",
    "\n",
    "\ttry:\n",
    "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "\t\tprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "\n",
    "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\t\ttpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
    "\n",
    "\t\tusing_tpu = True\n",
    "\texcept ValueError:\n",
    "\t\tprint(\"Note: Not connected to a TPU runtime.\")\n",
    "\t\tusing_tpu = False\n",
    "except ModuleNotFoundError:\n",
    "\traise Exception(\"Must be connected to Google Colab\")\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create All Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"yelp_dataset/yelp_academic_dataset_review.json\"\n",
    "write_path = \"yelp_dataset/all_reviews.json\"\n",
    "colab_path = \"drive/MyDrive/Colab Notebooks/yelp_dataset/all_reviews.json\"\n",
    "\n",
    "if Path(colab_path).exists():\n",
    "\tPath(\"yelp_dataset\").mkdir(exist_okay=True)\n",
    "\tshutil.copy(colab_path, write_path)\n",
    "else:\n",
    "\tstart_time = time.perf_counter()\n",
    "\t# Unzip yelp reviews and make all reviews\n",
    "\tif Path(\"yelp_dataset\").exists():\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\t# Extract tar file\n",
    "\t\tPath(\"yelp_dataset\").mkdir(exist_ok=True)\n",
    "\t\twith tarfile.open(\"drive/MyDrive/Colab Notebooks/yelp_dataset.tgz\") as tar:\n",
    "\t\t\ttar.extractall(\"yelp_dataset/\")\n",
    "\t\n",
    "\t# Read line by line as json, extract just the \"text\" and \"stars\", then write line by line to new json\n",
    "\twith open(read_path, \"r\") as yelp_review, open(write_path, \"w+\") as all_reviews:\n",
    "\t\ti = 1\n",
    "\t\tfor review in yelp_review:\n",
    "\t\t\tif i % 1000 == 0:\n",
    "\t\t\t\tprint(f\"Reading chunk {i} of 8636\", end=\"\\r\")\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\t\tline = json.loads(review)\n",
    "\t\t\treview_features = {\"text\": line[\"text\"], \"stars\": int(line[\"stars\"])}\n",
    "\t\t\tjson_string = json.dumps(review_features)\n",
    "\t\t\tall_reviews.write(json_string + \"\\n\")\n",
    "\n",
    "\tshutil.copy(write_path, colab_path)\n",
    "\tprint(\"All reviews saved to disk.\")\n",
    "\n",
    "\tprint(f\"Reviews processed in {round(time.perf_counter() - start_time)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset into memory\n",
    "review_df = pd.read_json(\"yelp_dataset/all_reviews.json\", orient=\"records\", lines=True)\n",
    "\n",
    "# Shuffle Review df\n",
    "review_df = shuffle(review_df, random_state=0)\n",
    "\n",
    "# Slice into Train, Val, Test at 60:20:20\n",
    "n = len(review_df)\n",
    "df_train = review_df.iloc[: int(n*0.6)]\n",
    "df_val = review_df.iloc[int(n*0.6) : int(n*0.8)]\n",
    "df_test = review_df.iloc[int(n*0.8) :]\n",
    "\n",
    "# Get ratios of stars\n",
    "initial_dist = df_train[\"stars\"].value_counts(normalize=True).sort_index().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DF to Balanced TF Dataset\n",
    "def class_func(features, label):\n",
    "\treturn label - 1\n",
    "\n",
    "def convert_text_df_to_dataset(df, input_col=\"text\", target_col=\"stars\"):\n",
    "\ttext_input = tf.convert_to_tensor(df[input_col], dtype=tf.string)\n",
    "\ttarget = tf.convert_to_tensor(df[target_col], dtype=tf.int8)\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices((text_input, target))\n",
    "\tdataset = dataset.rejection_resample(class_func, [0.2, 0.2, 0.2, 0.2, 0.2], initial_dist=initial_dist)\n",
    "\tdataset = dataset.map(lambda extra_label, features_and_label: features_and_label, num_parallel_calls=AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "train_dataset = convert_text_df_to_dataset(df_train)\n",
    "val_dataset = convert_text_df_to_dataset(df_val)\n",
    "test_dataset = convert_text_df_to_dataset(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram Vectorization - Categorical Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "# Create TextVectorization\n",
    "max_tokens = 50000\n",
    "text_vectorization_ngram = TextVectorization(max_tokens=max_tokens, ngrams=3, output_mode=\"multi_hot\")\n",
    "\n",
    "# Train Vectorizer on train text\n",
    "text_vectorization_ngram.adapt(df_train[\"text\"], batch_size=2**16)\n",
    "\n",
    "print(f\"Created text vectorization in {round(time.perf_counter() - start_time)} seconds. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Dataset feedthrogh\n",
    "if using_tpu:\n",
    "\t# TPU's really like big batches I guess. \n",
    "\t# By increasing the batch size by a factor of 128, I am seeing about a 4x speedup. \n",
    "\tbatch_size = 16 * 128 * tpu_strategy.num_replicas_in_sync\n",
    "else:\n",
    "\tbatch_size = 128\n",
    "\n",
    "num_train_epochs = 5 * min(initial_dist) * len(df_train) // batch_size\n",
    "\n",
    "# Vectorize Datasets, shift labels\n",
    "train_dataset_vectorized = train_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_ngram(x), y-1), num_parallel_calls=AUTO)\n",
    "val_dataset_vectorized = val_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_ngram(x), y-1), num_parallel_calls=AUTO)\n",
    "test_dataset_vectorized = test_dataset.batch(batch_size).map(lambda x, y: (text_vectorization_ngram(x), y-1), num_parallel_calls=AUTO)\n",
    "\n",
    "# Repeat train dataset, and prefetch all datasets\n",
    "train_dataset_vectorized = train_dataset_vectorized.repeat().prefetch(num_train_epochs)\n",
    "val_dataset_vectorized = val_dataset_vectorized.prefetch(AUTO)\n",
    "test_dataset_vectorized = test_dataset_vectorized.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def create_model_categorical(max_tokens, model_name):\n",
    "\tinputs = keras.Input(shape=(max_tokens,))\n",
    "\tx = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "\tx = keras.layers.Dropout(0.25)(x)\n",
    "\tx = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "\tx = keras.layers.Dropout(0.25)(x)\n",
    "\toutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "\tmodel = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=\"rmsprop\", \n",
    "\t\tloss=\"sparse_categorical_crossentropy\", \n",
    "\t\tmetrics=[\"sparse_categorical_accuracy\"]\n",
    "\t)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"large_ngram_model\"\n",
    "\n",
    "# Creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "if using_tpu:\n",
    "\twith tpu_strategy.scope():\n",
    "\t\tmodel = create_model_categorical(max_tokens, model_name)\n",
    "else:\n",
    "\tmodel = create_model_categorical(max_tokens, model_name)\n",
    "\n",
    "# Create callback to save model with a given name\n",
    "model_path = f\"models/{model_name}.keras\"\n",
    "callbacks = [\n",
    "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
    "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=False)\n",
    "]\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Train Model\n",
    "model.fit(train_dataset_vectorized, \n",
    "\tvalidation_data=val_dataset_vectorized, \n",
    "\tsteps_per_epoch=num_train_epochs, \n",
    "\tepochs=30, \n",
    "\tcallbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model after training\n",
    "model = keras.models.load_model(model_path)\n",
    "predictions = model.predict(test_dataset_vectorized)\n",
    "predictions = np.argmax(predictions, axis = -1)\n",
    "true_labels = np.concatenate([y for _, y in test_dataset_vectorized], axis=0)\n",
    "mae = mean_absolute_error(true_labels, predictions)\n",
    "mse = mean_squared_error(true_labels, predictions)\n",
    "\n",
    "mins, secs = divmod(time.perf_counter() - start_time, 60)\n",
    "# Output Model Metrics\n",
    "print(f\"Trained model in {mins} minutes, {round(secs)} seconds\")\n",
    "metrics_text = f\"Model {model_name} with MAE {mae:.3f} and MSE {mse:.3f}\\n\"\n",
    "print(metrics_text)\n",
    "with open(\"model_metrics.txt\", \"a\") as f:\n",
    "\tf.write(metrics_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model with Text Vectorization layer\n",
    "inputs = keras.Inputs(shape=(1,), dtype=\"string\")\n",
    "vectorized_inputs = text_vectorization_ngram(inputs)\n",
    "outputs = model(vectorized_inputs)\n",
    "\n",
    "inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "keras.models.save_model(inference_model, \"models/full_text_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy models to Google Drive to ensure that they are not lost\n",
    "shutil.copytree(\"models\", \"drive/MyDrive/Colab Notebooks/models\")\n",
    "shutil.copy(\"model_metrics.txt\", \"drive/MyDrive/Colab Notebooks/model_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip Models\n",
    "!zip -r \"models.zip\" \"models\"\n",
    "\n",
    "try: \n",
    "\tfrom google.colab import files\n",
    "\tfiles.download(\"models.zip\")\n",
    "\tfiles.download(\"model_metrics.txt\")\n",
    "except:\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "review_model = keras.models.load_model(\"models/full_text_model.keras\")\n",
    "\n",
    "review_text = [\n",
    "\t[\"I think my meal was decent, but I have had better. I would recommend other places in the area.\"],\n",
    "\t[\"My meal was excellent, and I had a really great time dining at this restaurant tonight. I will be back!\"], \n",
    "\t[\"Horrible experience. The food was awful and I wish to never return to this restaurant again.\"]\n",
    "]\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor(review_text)\n",
    "\n",
    "predictions = review_model(raw_text_data)\n",
    "predictions = np.argmax(predictions, axis = -1) + 1\n",
    "for text, star in zip(review_text, predictions):\n",
    "\tprint(f\"{star}: {review_text}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
