{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFiVtdVhRGIi"
      },
      "source": [
        "# Train Large Vectorized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "edKtoIMbRGIk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import time\n",
        "import tarfile\n",
        "import json\n",
        "import math\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RErtpmxlRGIm"
      },
      "source": [
        "## Connect to Colab GPU\n",
        "\n",
        "Note: Can not use TF Records with TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pwgYMF1-RGIm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.7.0\n",
            "Not connected to Google Colab\n"
          ]
        }
      ],
      "source": [
        "# Check if Google Colab Instance for Setup\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Get correct path if on Google Colab\n",
        "try:\n",
        "\tfrom google.colab import drive\n",
        "\tdrive.mount(\"/content/drive\")\n",
        "\n",
        "\t# Get RAM Info\n",
        "\tfrom psutil import virtual_memory\n",
        "\tram_gb = virtual_memory().total / 1e9\n",
        "\tprint('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "\tif ram_gb < 20:\n",
        "\t\tprint('Not using a high-RAM runtime')\n",
        "\telse:\n",
        "\t\tprint('You are using a high-RAM runtime!')\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\tprint(\"Not connected to Google Colab\")\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVkGVD-IRGIn"
      },
      "source": [
        "## Create TF Records from Review JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tYYtVUfDRGIn"
      },
      "outputs": [],
      "source": [
        "# https://keras.io/examples/keras_recipes/creating_tfrecords/\n",
        "\n",
        "def bytes_feature(value):\n",
        "\t\"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
        "\n",
        "def int64_feature(value):\n",
        "\t\"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "\treturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def create_example(text, stars):\n",
        "\tfeature = {\n",
        "\t\t\"text\": bytes_feature(text),\n",
        "\t\t\"stars\": int64_feature(stars)\n",
        "\t}\n",
        "\treturn tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "\n",
        "def parse_tfrecord_fn(example):\n",
        "\tfeature_description = {\n",
        "\t\t\"text\": tf.io.FixedLenFeature([], tf.string),\n",
        "\t\t\"stars\": tf.io.FixedLenFeature([], tf.int64),\n",
        "\t}\n",
        "\texample = tf.io.parse_single_example(example, feature_description)\n",
        "\treturn example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ShardedWriter():\n",
        "\tdef __init__(self, parent_dir, num_example_per_record, num_classes, class_labels=None):\n",
        "\t\tself.parent_dir = pathlib.Path(parent_dir)\n",
        "\t\tself.num_example_per_record = num_example_per_record\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tif class_labels is None:\n",
        "\t\t\tself.class_labels = list(range(self.num_classes))\n",
        "\t\telse:\n",
        "\t\t\tassert num_classes == len(class_labels), \"num_classes does not match the number of class_labels\"\n",
        "\t\t\tself.class_labels = class_labels\n",
        "\t\tself.record_counter = [1 for _ in range(self.num_classes)]\n",
        "\t\tself.element_counter = [0 for _ in range(self.num_classes)]\n",
        "\t\tself._init_paths()\n",
        "\t\tself.writers = self._init_writers()\n",
        "\n",
        "\tdef _init_paths(self):\n",
        "\t\t# Make sure all the directories exist before making writers. \n",
        "\t\tfor label in self.class_labels:\n",
        "\t\t\tpath_name = self.parent_dir / str(label)\n",
        "\t\t\tpath_name.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\tdef _make_writer(self, path_name):\n",
        "\t\t# Create a TFRecord writer.\n",
        "\t\tpath_name = str(path_name)\n",
        "\t\treturn tf.io.TFRecordWriter(path_name)\n",
        "\n",
        "\tdef _init_writers(self):\n",
        "\t\t# Initialize a new set of writers. Should only be called during class init. \n",
        "\t\twriters = []\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\trecord_name = f\"01.tfrecord\"\n",
        "\t\t\tpath_name = self.parent_dir / str(self.class_labels[i]) / record_name\n",
        "\t\t\twriter = self._make_writer(path_name)\n",
        "\t\t\twriters.append(writer)\n",
        "\t\treturn writers\n",
        "\t\n",
        "\tdef _get_new_writer(self, index):\n",
        "\t\t# Close the current writer and create a new one, incrementing the file name\n",
        "\t\t# Close previous writer\n",
        "\t\tself.writers[index].close()\n",
        "\n",
        "\t\t# Increment file name counter\n",
        "\t\tself.record_counter[index] += 1\n",
        "\t\trecord_name = f\"{self.record_counter[index]:02}.tfrecord\"\n",
        "\t\tpath_name = self.parent_dir / str(self.class_labels[index]) / record_name\n",
        "\n",
        "\t\t# Make writer\n",
        "\t\tself.writers[index] = self._make_writer(path_name)\n",
        "\t\n",
        "\tdef write_example(self, example, label):\n",
        "\t\t# Know which class the example belongs to based on label\n",
        "\t\tif isinstance(label, int):\n",
        "\t\t\tclass_index = label\n",
        "\t\telse:\n",
        "\t\t\tclass_index = self.class_labels.index(label)\n",
        "\n",
        "\t\t# Check element_counter\n",
        "\t\tif self.element_counter[class_index] >= self.num_example_per_record:\n",
        "\t\t\tself._get_new_writer(class_index)\n",
        "\t\t\tself.element_counter[class_index] = 0\n",
        "\t\telse:\n",
        "\t\t\tself.element_counter[class_index] += 1\n",
        "\n",
        "\t\tself.writers[class_index].write(example)\n",
        "\n",
        "\tdef get_record_paths(self):\n",
        "\t\t# Get a list of all the record paths for each class. \n",
        "\t\trecord_paths = []\n",
        "\t\tfor i, label in enumerate(self.class_labels):\n",
        "\t\t\tlabel_paths = []\n",
        "\t\t\tfor j in range(1, self.record_counter[j] + 1):\n",
        "\t\t\t\trecord_name = f\"{j:02}.tfrecord\"\n",
        "\t\t\t\tpath_name = self.parent_dir / str(self.class_labels[i]) / record_name\n",
        "\t\t\t\tlabel_paths.append(path_name)\n",
        "\t\t\trecord_paths.append(label_paths)\n",
        "\t\treturn record_paths\n",
        "\n",
        "\tdef get_counts(self):\n",
        "\t\t# Get total counts of each class encountered and saved. \n",
        "\t\tcounts = [(self.record_counter[i] - 1) * self.num_example_per_record + self.element_counter[i] for i in range(self.num_classes)]\n",
        "\t\treturn counts\n",
        "\n",
        "\tdef close_all_writers(self):\n",
        "\t\tfor writer in self.writers:\n",
        "\t\t\twriter.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "IzlXdC5PRGIo",
        "outputId": "5707e220-adcd-4009-bd04-c4569e08f8dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='8635403' class='' max='8635403' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [8635403/8635403 10:26<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 1262781 elements of rating 1.\n",
            "Saved 711368 elements of rating 2.\n",
            "Saved 926642 elements of rating 3.\n",
            "Saved 1920008 elements of rating 4.\n",
            "Saved 3814474 elements of rating 5.\n",
            "All reviews saved to disk.\n",
            "Reviews processed in 1118 seconds.\n"
          ]
        }
      ],
      "source": [
        "read_path = \"yelp_dataset/yelp_academic_dataset_review.json\"\n",
        "write_path = \"yelp_dataset/all_reviews.zip\"\n",
        "colab_path = \"drive/MyDrive/Colab Notebooks/yelp_dataset/all_reviews.zip\"\n",
        "\n",
        "if Path(\"yelp_dataset/all_reviews\").exists():\n",
        "\t# Data should already be in place\n",
        "\tpass\n",
        "elif Path(colab_path).exists():\n",
        "\tPath(\"yelp_dataset\").mkdir(exist_ok=True)\n",
        "\tshutil.copy(colab_path, \"yelp_dataset/\")\n",
        "\tPath(\"yelp_dataset/all_reviews\").mkdir(exist_ok=True)\n",
        "\tshutil.unpack_archive(write_path, \"yelp_dataset/all_reviews\", \"zip\")\n",
        "else:\n",
        "\tstart_time = time.perf_counter()\n",
        "\t# Unzip yelp reviews and make all reviews\n",
        "\tif Path(\"yelp_dataset\").exists():\n",
        "\t\tpass\n",
        "\telse:\n",
        "\t\t# Extract tar file\n",
        "\t\tPath(\"yelp_dataset\").mkdir(exist_ok=True)\n",
        "\t\twith tarfile.open(\"drive/MyDrive/Colab Notebooks/yelp_dataset.tgz\") as tar:\n",
        "\t\t\ttar.extractall(\"yelp_dataset/\")\n",
        "\n",
        "\t# Number of Reviews is known\n",
        "\tNUM_REVIEWS = 8_635_403\n",
        "\n",
        "\t# Make a writer for each star category\n",
        "\tparent_dir = Path(\"yelp_dataset/all_reviews\")\n",
        "\tparent_dir.mkdir(exist_ok=True)\n",
        "\t# writers = [tf.io.TFRecordWriter(f\"yelp_dataset/all_reviews/all_{i}_star_reviews.tfrecord\") for i in range(1, 6)]\n",
        "\tclass_labels = [\"1_star\", \"2_star\", \"3_star\", \"4_star\", \"5_star\"]\n",
        "\tsharded_writer = ShardedWriter(parent_dir, num_example_per_record=2**16, num_classes=5, class_labels=class_labels)\n",
        "\n",
        "\t# Read line by line as json, extract just the \"text\" and \"stars\", then write line by line to new json\n",
        "\twith open(read_path, \"r\") as yelp_review:\n",
        "\t\tfor i in progress_bar(range(NUM_REVIEWS)):\n",
        "\t\t\t# Get next line of JSON\n",
        "\t\t\treview = next(yelp_review)\n",
        "\t\t\t# Parse JSON\n",
        "\t\t\tline = json.loads(review)\n",
        "\t\t\t# Create tf.train.Example\n",
        "\t\t\texample = create_example(line[\"text\"], int(line[\"stars\"]))\n",
        "\t\t\t# Write TF Record to correct record\n",
        "\t\t\tsharded_writer.write_example(example, label=int(line[\"stars\"])-1)\n",
        "\n",
        "\t# Close all TF Record writers\n",
        "\tsharded_writer.close_all_writers()\n",
        "\n",
        "\t# Get counts\n",
        "\tcounts = sharded_writer.get_counts()\n",
        "\tfor i, count in enumerate(counts):\n",
        "\t\tprint(f\"Saved {count} elements of rating {i + 1}.\")\n",
        "\n",
        "\t# Zip all review files\n",
        "\tshutil.make_archive(\"yelp_dataset/all_reviews\", \"zip\", \"yelp_dataset/all_reviews\")\n",
        "\n",
        "\tPath(\"drive/MyDrive/Colab Notebooks/yelp_dataset\").mkdir(parents=True, exist_ok=True)\n",
        "\tshutil.copy(write_path, \"drive/MyDrive/Colab Notebooks/yelp_dataset\")\n",
        "\tprint(\"All reviews saved to disk.\")\n",
        "\n",
        "\tprint(f\"Reviews processed in {round(time.perf_counter() - start_time)} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load All TF Records into balanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pathnames\n",
        "record_paths = {}\n",
        "parent_dir = Path(\"yelp_dataset/all_reviews\")\n",
        "for class_dir in parent_dir.iterdir():\n",
        "\tif class_dir.is_dir():\n",
        "\t\trecord_paths[class_dir.parts[-1]] = []\n",
        "\t\tfor file_name in class_dir.iterdir():\n",
        "\t\t\tif file_name.suffix == \".tfrecord\":\n",
        "\t\t\t\trecord_paths[file_name.parts[-2]].append(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select Train Val Test\n",
        "# Test and Val have 20% each\n",
        "test_paths = {}\n",
        "val_paths = {}\n",
        "train_paths = {}\n",
        "for key, value in record_paths.items():\n",
        "\tvalue.sort(key=lambda path: int(path.stem))\n",
        "\ttest_paths[key] = value[4::5]\n",
        "\tval_paths[key] = value[3::5]\n",
        "\ttrain_paths[key] = value[2::5] + value[1::5] + value[0::5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods to construct datasets\n",
        "def prepare_sample(example):\n",
        "\t# Map to star index\n",
        "\treturn example[\"text\"], int(example[\"stars\"] - 1)\n",
        "\n",
        "def construct_dataset(path_dict):\n",
        "\t# Construct Datasets\n",
        "\traw_datasets = [tf.data.TFRecordDataset(path_dict[f\"{i}_star\"], num_parallel_reads=AUTO) for i in range(1, 6)]\n",
        "\n",
        "\t# Create a balanced dataset by evenly selecting from each dataset\n",
        "\treviews_ds = tf.data.Dataset.sample_from_datasets(\n",
        "\t\tdatasets=raw_datasets, \n",
        "\t\tweights=[0.2, 0.2, 0.2, 0.2, 0.2], \n",
        "\t\tseed=0, \n",
        "\t\tstop_on_empty_dataset=True\n",
        "\t)\n",
        "\n",
        "\t# Parse all records\n",
        "\treviews_ds = reviews_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
        "\t# Convert ratings to zero index\n",
        "\treviews_ds = reviews_ds.map(prepare_sample, num_parallel_calls=AUTO)\n",
        "\n",
        "\treturn reviews_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_ds = construct_dataset(train_paths)\n",
        "val_ds = construct_dataset(val_paths)\n",
        "test_ds = construct_dataset(test_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_reviews = [1262801, 711379, 926657, 1920038, 3814533]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tSO4C2RGIr"
      },
      "source": [
        "## Sequence Embedded Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.perf_counter()\n",
        "# Create TextVectorization\n",
        "max_tokens = 30000\n",
        "max_length = 500\n",
        "batch_size = 2 ** 12\n",
        "text_vectorization = TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
        "\n",
        "# Train Vectorizer on train text\n",
        "text_vectorization.adapt(train_ds.map(lambda x, y: x, num_parallel_calls=AUTO).batch(batch_size))\n",
        "\n",
        "print(f\"Created text vectorization in {round(time.perf_counter() - start_time)} seconds. \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlRDHm3ZRGIr"
      },
      "outputs": [],
      "source": [
        "# Vectorize Datasets\n",
        "train_dataset_vectorized = train_ds.batch(batch_size).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "val_dataset_vectorized = val_ds.batch(batch_size).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "test_dataset_vectorized = test_ds.batch(batch_size).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "\n",
        "# Repeat train dataset, and prefetch all datasets\n",
        "train_dataset_vectorized = train_dataset_vectorized.prefetch(AUTO)\n",
        "val_dataset_vectorized = val_dataset_vectorized.prefetch(AUTO)\n",
        "test_dataset_vectorized = test_dataset_vectorized.prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-s55OyHRGIs"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def create_embedding_model_categorical(max_tokens, model_name):\n",
        "\tinputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\tembedded = keras.layers.Embedding(input_dim=max_tokens, output_dim=512, mask_zero=True)(inputs)\n",
        "\tx = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(embedded)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\tx = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\toutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "\tmodel = keras.Model(inputs, outputs, name=model_name)\n",
        "\n",
        "\tmodel.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49a0JkYARGIs"
      },
      "outputs": [],
      "source": [
        "model_name = \"large_sequence_embedded_model\"\n",
        "\n",
        "model = create_embedding_model_categorical(max_tokens, model_name)\n",
        "\n",
        "# Create callback to save model with a given name\n",
        "model_path = f\"models/{model_name}.keras\"\n",
        "callbacks = [\n",
        "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
        "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=False)\n",
        "]\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Train Model\n",
        "model.fit(train_dataset_vectorized.cache(), \n",
        "\tvalidation_data=val_dataset_vectorized.cache(), \n",
        "\t# steps_per_epoch=num_train_epochs, \n",
        "\tepochs=1, \n",
        "\tcallbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate Model after training\n",
        "model = keras.models.load_model(model_path)\n",
        "predictions = model.predict(test_dataset_vectorized)\n",
        "predictions = np.argmax(predictions, axis = -1)\n",
        "true_labels = np.concatenate([y for _, y in test_dataset_vectorized], axis=0)\n",
        "mae = mean_absolute_error(true_labels, predictions)\n",
        "mse = mean_squared_error(true_labels, predictions)\n",
        "\n",
        "mins, secs = divmod(time.perf_counter() - start_time, 60)\n",
        "# Output Model Metrics\n",
        "print(f\"Trained model in {mins} minutes, {round(secs)} seconds\")\n",
        "metrics_text = f\"Model {model_name} with MAE {mae:.3f} and MSE {mse:.3f}\\n\"\n",
        "print(metrics_text)\n",
        "with open(\"model_metrics.txt\", \"a\") as f:\n",
        "\tf.write(metrics_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHnOGfCSRGIt"
      },
      "outputs": [],
      "source": [
        "# Export model with Text Vectorization layer\n",
        "inputs = keras.Inputs(shape=(1,), dtype=\"string\")\n",
        "vectorized_inputs = text_vectorization(inputs)\n",
        "outputs = model(vectorized_inputs)\n",
        "\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "\n",
        "keras.models.save_model(inference_model, \"models/full_text_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25MLzAoJRGIu"
      },
      "outputs": [],
      "source": [
        "# Zip Models\n",
        "!zip -r \"models.zip\" \"models\"\n",
        "\n",
        "try: \n",
        "\tfrom google.colab import files\n",
        "\tfiles.download(\"models.zip\")\n",
        "\tfiles.download(\"model_metrics.txt\")\n",
        "except:\n",
        "\tpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMYnYjRtRGIu"
      },
      "outputs": [],
      "source": [
        "# Test model\n",
        "review_model = keras.models.load_model(\"models/full_text_model.keras\")\n",
        "\n",
        "review_text = [\n",
        "\t[\"I think my meal was decent, but I have had better. I would recommend other places in the area.\"],\n",
        "\t[\"My meal was excellent, and I had a really great time dining at this restaurant tonight. I will be back!\"], \n",
        "\t[\"Horrible experience. The food was awful and I wish to never return to this restaurant again.\"]\n",
        "]\n",
        "\n",
        "raw_text_data = tf.convert_to_tensor(review_text)\n",
        "\n",
        "predictions = review_model(raw_text_data)\n",
        "predictions = np.argmax(predictions, axis = -1) + 1\n",
        "for text, star in zip(review_text, predictions):\n",
        "\tprint(f\"{star}: {review_text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_large_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
