{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFiVtdVhRGIi"
      },
      "source": [
        "# Train Large Vectorized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "edKtoIMbRGIk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import time\n",
        "import tarfile\n",
        "import json\n",
        "import math\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RErtpmxlRGIm"
      },
      "source": [
        "## Connect to Colab GPU\n",
        "\n",
        "Note: Can not use TF Records with TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pwgYMF1-RGIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c9cb33-8d5d-48b0-928a-ed5904eb29c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.7.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# Check if Google Colab Instance for Setup\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Get correct path if on Google Colab\n",
        "try:\n",
        "\tfrom google.colab import drive\n",
        "\tdrive.mount(\"/content/drive\")\n",
        "\n",
        "\t# Get RAM Info\n",
        "\tfrom psutil import virtual_memory\n",
        "\tram_gb = virtual_memory().total / 1e9\n",
        "\tprint('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "\tif ram_gb < 20:\n",
        "\t\tprint('Not using a high-RAM runtime')\n",
        "\telse:\n",
        "\t\tprint('You are using a high-RAM runtime!')\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\tprint(\"Not connected to Google Colab\")\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVkGVD-IRGIn"
      },
      "source": [
        "## Create TF Records from Review JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tYYtVUfDRGIn"
      },
      "outputs": [],
      "source": [
        "# https://keras.io/examples/keras_recipes/creating_tfrecords/\n",
        "\n",
        "def bytes_feature(value):\n",
        "\t\"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
        "\n",
        "def int64_feature(value):\n",
        "\t\"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "\treturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def create_example(text, stars):\n",
        "\tfeature = {\n",
        "\t\t\"text\": bytes_feature(text),\n",
        "\t\t\"stars\": int64_feature(stars)\n",
        "\t}\n",
        "\treturn tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "\n",
        "def parse_tfrecord_fn(example):\n",
        "\tfeature_description = {\n",
        "\t\t\"text\": tf.io.FixedLenFeature([], tf.string),\n",
        "\t\t\"stars\": tf.io.FixedLenFeature([], tf.int64),\n",
        "\t}\n",
        "\texample = tf.io.parse_single_example(example, feature_description)\n",
        "\treturn example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cKw_Q9A8cKIf"
      },
      "outputs": [],
      "source": [
        "class ShardedWriter():\n",
        "\tdef __init__(self, parent_dir, num_example_per_record, num_classes, class_labels=None):\n",
        "\t\tself.parent_dir = pathlib.Path(parent_dir)\n",
        "\t\tself.num_example_per_record = num_example_per_record\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tif class_labels is None:\n",
        "\t\t\tself.class_labels = list(range(self.num_classes))\n",
        "\t\telse:\n",
        "\t\t\tassert num_classes == len(class_labels), \"num_classes does not match the number of class_labels\"\n",
        "\t\t\tself.class_labels = class_labels\n",
        "\t\tself.record_counter = [1 for _ in range(self.num_classes)]\n",
        "\t\tself.element_counter = [0 for _ in range(self.num_classes)]\n",
        "\t\tself._init_paths()\n",
        "\t\tself.writers = self._init_writers()\n",
        "\n",
        "\tdef _init_paths(self):\n",
        "\t\t# Make sure all the directories exist before making writers. \n",
        "\t\tfor label in self.class_labels:\n",
        "\t\t\tpath_name = self.parent_dir / str(label)\n",
        "\t\t\tpath_name.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\tdef _make_writer(self, path_name):\n",
        "\t\t# Create a TFRecord writer.\n",
        "\t\tpath_name = str(path_name)\n",
        "\t\treturn tf.io.TFRecordWriter(path_name)\n",
        "\n",
        "\tdef _init_writers(self):\n",
        "\t\t# Initialize a new set of writers. Should only be called during class init. \n",
        "\t\twriters = []\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\trecord_name = f\"01.tfrecord\"\n",
        "\t\t\tpath_name = self.parent_dir / str(self.class_labels[i]) / record_name\n",
        "\t\t\twriter = self._make_writer(path_name)\n",
        "\t\t\twriters.append(writer)\n",
        "\t\treturn writers\n",
        "\t\n",
        "\tdef _get_new_writer(self, index):\n",
        "\t\t# Close the current writer and create a new one, incrementing the file name\n",
        "\t\t# Close previous writer\n",
        "\t\tself.writers[index].close()\n",
        "\n",
        "\t\t# Increment file name counter\n",
        "\t\tself.record_counter[index] += 1\n",
        "\t\trecord_name = f\"{self.record_counter[index]:02}.tfrecord\"\n",
        "\t\tpath_name = self.parent_dir / str(self.class_labels[index]) / record_name\n",
        "\n",
        "\t\t# Make writer\n",
        "\t\tself.writers[index] = self._make_writer(path_name)\n",
        "\t\n",
        "\tdef write_example(self, example, label):\n",
        "\t\t# Know which class the example belongs to based on label\n",
        "\t\tif isinstance(label, int):\n",
        "\t\t\tclass_index = label\n",
        "\t\telse:\n",
        "\t\t\tclass_index = self.class_labels.index(label)\n",
        "\n",
        "\t\t# Check element_counter\n",
        "\t\tif self.element_counter[class_index] >= self.num_example_per_record:\n",
        "\t\t\tself._get_new_writer(class_index)\n",
        "\t\t\tself.element_counter[class_index] = 0\n",
        "\t\telse:\n",
        "\t\t\tself.element_counter[class_index] += 1\n",
        "\n",
        "\t\tself.writers[class_index].write(example)\n",
        "\n",
        "\tdef get_record_paths(self):\n",
        "\t\t# Get a list of all the record paths for each class. \n",
        "\t\trecord_paths = []\n",
        "\t\tfor i, label in enumerate(self.class_labels):\n",
        "\t\t\tlabel_paths = []\n",
        "\t\t\tfor j in range(1, self.record_counter[j] + 1):\n",
        "\t\t\t\trecord_name = f\"{j:02}.tfrecord\"\n",
        "\t\t\t\tpath_name = self.parent_dir / str(self.class_labels[i]) / record_name\n",
        "\t\t\t\tlabel_paths.append(path_name)\n",
        "\t\t\trecord_paths.append(label_paths)\n",
        "\t\treturn record_paths\n",
        "\n",
        "\tdef get_counts(self):\n",
        "\t\t# Get total counts of each class encountered and saved. \n",
        "\t\tcounts = [(self.record_counter[i] - 1) * self.num_example_per_record + self.element_counter[i] for i in range(self.num_classes)]\n",
        "\t\treturn counts\n",
        "\n",
        "\tdef close_all_writers(self):\n",
        "\t\tfor writer in self.writers:\n",
        "\t\t\twriter.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IzlXdC5PRGIo"
      },
      "outputs": [],
      "source": [
        "read_path = \"yelp_dataset/yelp_academic_dataset_review.json\"\n",
        "write_path = \"yelp_dataset/all_reviews.zip\"\n",
        "colab_path = \"drive/MyDrive/Colab Notebooks/yelp_dataset/all_reviews.zip\"\n",
        "\n",
        "if Path(\"yelp_dataset/all_reviews\").exists():\n",
        "\t# Data should already be in place\n",
        "\tpass\n",
        "elif Path(colab_path).exists():\n",
        "\tPath(\"yelp_dataset\").mkdir(exist_ok=True)\n",
        "\tshutil.copy(colab_path, \"yelp_dataset/\")\n",
        "\tPath(\"yelp_dataset/all_reviews\").mkdir(exist_ok=True)\n",
        "\tshutil.unpack_archive(write_path, \"yelp_dataset/all_reviews\", \"zip\")\n",
        "else:\n",
        "\tstart_time = time.perf_counter()\n",
        "\t# Unzip yelp reviews and make all reviews\n",
        "\tif Path(\"yelp_dataset\").exists():\n",
        "\t\tpass\n",
        "\telse:\n",
        "\t\t# Extract tar file\n",
        "\t\tPath(\"yelp_dataset\").mkdir(exist_ok=True)\n",
        "\t\twith tarfile.open(\"drive/MyDrive/Colab Notebooks/yelp_dataset.tgz\") as tar:\n",
        "\t\t\ttar.extractall(\"yelp_dataset/\")\n",
        "\n",
        "\t# Number of Reviews is known\n",
        "\tNUM_REVIEWS = 8_635_403\n",
        "\n",
        "\t# Make a writer for each star category\n",
        "\tparent_dir = Path(\"yelp_dataset/all_reviews\")\n",
        "\tparent_dir.mkdir(exist_ok=True)\n",
        "\t# writers = [tf.io.TFRecordWriter(f\"yelp_dataset/all_reviews/all_{i}_star_reviews.tfrecord\") for i in range(1, 6)]\n",
        "\tclass_labels = [\"1_star\", \"2_star\", \"3_star\", \"4_star\", \"5_star\"]\n",
        "\tsharded_writer = ShardedWriter(parent_dir, num_example_per_record=2**16, num_classes=5, class_labels=class_labels)\n",
        "\n",
        "\t# Read line by line as json, extract just the \"text\" and \"stars\", then write line by line to new json\n",
        "\twith open(read_path, \"r\") as yelp_review:\n",
        "\t\tfor i in progress_bar(range(NUM_REVIEWS)):\n",
        "\t\t\t# Get next line of JSON\n",
        "\t\t\treview = next(yelp_review)\n",
        "\t\t\t# Parse JSON\n",
        "\t\t\tline = json.loads(review)\n",
        "\t\t\t# Create tf.train.Example\n",
        "\t\t\texample = create_example(line[\"text\"], int(line[\"stars\"]))\n",
        "\t\t\t# Write TF Record to correct record\n",
        "\t\t\tsharded_writer.write_example(example, label=int(line[\"stars\"])-1)\n",
        "\n",
        "\t# Close all TF Record writers\n",
        "\tsharded_writer.close_all_writers()\n",
        "\n",
        "\t# Get counts\n",
        "\tcounts = sharded_writer.get_counts()\n",
        "\tfor i, count in enumerate(counts):\n",
        "\t\tprint(f\"Saved {count} elements of rating {i + 1}.\")\n",
        "\n",
        "\t# Zip all review files\n",
        "\tshutil.make_archive(\"yelp_dataset/all_reviews\", \"zip\", \"yelp_dataset/all_reviews\")\n",
        "\n",
        "\tPath(\"drive/MyDrive/Colab Notebooks/yelp_dataset\").mkdir(parents=True, exist_ok=True)\n",
        "\tshutil.copy(write_path, \"drive/MyDrive/Colab Notebooks/yelp_dataset\")\n",
        "\tprint(\"All reviews saved to disk.\")\n",
        "\n",
        "\tprint(f\"Reviews processed in {round(time.perf_counter() - start_time)} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNc3LLf9cKIi"
      },
      "source": [
        "## Load All TF Records into balanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ji1EAw2icKIj"
      },
      "outputs": [],
      "source": [
        "# Get pathnames\n",
        "record_paths = {}\n",
        "parent_dir = Path(\"yelp_dataset/all_reviews\")\n",
        "for class_dir in parent_dir.iterdir():\n",
        "\tif class_dir.is_dir():\n",
        "\t\trecord_paths[class_dir.parts[-1]] = []\n",
        "\t\tfor file_name in class_dir.iterdir():\n",
        "\t\t\tif file_name.suffix == \".tfrecord\":\n",
        "\t\t\t\trecord_paths[file_name.parts[-2]].append(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bxDhs_SHcKIk"
      },
      "outputs": [],
      "source": [
        "# Select Train Val Test\n",
        "# Test and Val have 20% each\n",
        "test_paths = {}\n",
        "val_paths = {}\n",
        "train_paths = {}\n",
        "for key, value in record_paths.items():\n",
        "\tvalue.sort(key=lambda path: int(path.stem))\n",
        "\ttest_paths[key] = value[4::5]\n",
        "\tval_paths[key] = value[3::5]\n",
        "\ttrain_paths[key] = value[2::5] + value[1::5] + value[0::5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P6mcffDxcKIk"
      },
      "outputs": [],
      "source": [
        "# Methods to construct datasets\n",
        "def prepare_sample(example):\n",
        "\t# Map to star index\n",
        "\treturn example[\"text\"], int(example[\"stars\"] - 1)\n",
        "\n",
        "def construct_dataset(path_dict):\n",
        "\t# Construct Datasets\n",
        "\traw_datasets = [tf.data.TFRecordDataset(path_dict[f\"{i}_star\"], num_parallel_reads=AUTO) for i in range(1, 6)]\n",
        "\n",
        "\t# Create a balanced dataset by evenly selecting from each dataset\n",
        "\treviews_ds = tf.data.Dataset.sample_from_datasets(\n",
        "\t\tdatasets=raw_datasets, \n",
        "\t\tweights=[0.2, 0.2, 0.2, 0.2, 0.2], \n",
        "\t\tseed=0, \n",
        "\t\tstop_on_empty_dataset=True\n",
        "\t)\n",
        "\n",
        "\t# Parse all records\n",
        "\treviews_ds = reviews_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
        "\t# Convert ratings to zero index\n",
        "\treviews_ds = reviews_ds.map(prepare_sample, num_parallel_calls=AUTO)\n",
        "\n",
        "\treturn reviews_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a_QYYDjJcKIl"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_ds = construct_dataset(train_paths)\n",
        "val_ds = construct_dataset(val_paths)\n",
        "test_ds = construct_dataset(test_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wtotPfaYcKIl"
      },
      "outputs": [],
      "source": [
        "num_reviews = [1262801, 711379, 926657, 1920038, 3814533]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tSO4C2RGIr"
      },
      "source": [
        "## Sequence Embedded Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUg1iUncKIm",
        "outputId": "bb3c4122-0ad7-4511-a3ea-99710572085b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created text vectorization in 202 seconds. \n"
          ]
        }
      ],
      "source": [
        "max_tokens = 20000\n",
        "max_length = 300\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "# Create TextVectorization\n",
        "text_vectorization = TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
        "\n",
        "# Train Vectorizer on train text\n",
        "text_vectorization.adapt(train_ds.map(lambda x, y: x, num_parallel_calls=AUTO).batch(256))\n",
        "\n",
        "print(f\"Created text vectorization in {round(time.perf_counter() - start_time)} seconds. \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NlRDHm3ZRGIr"
      },
      "outputs": [],
      "source": [
        "batch_size = 2 ** 10\n",
        "# Vectorize Datasets\n",
        "train_dataset_vectorized = train_ds.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "val_dataset_vectorized = val_ds.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "test_dataset_vectorized = test_ds.batch(batch_size, drop_remainder=True, num_parallel_calls=AUTO).map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=AUTO)\n",
        "\n",
        "# Repeat train dataset, and prefetch all datasets\n",
        "train_dataset_vectorized = train_dataset_vectorized.prefetch(1000)\n",
        "val_dataset_vectorized = val_dataset_vectorized.prefetch(AUTO)\n",
        "test_dataset_vectorized = test_dataset_vectorized.prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-s55OyHRGIs"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pZbM43l2cKIn"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def create_embedding_model_categorical(max_tokens, model_name):\n",
        "\tinputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n",
        "\tembedded = keras.layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "\tx = keras.layers.Bidirectional(keras.layers.LSTM(32))(embedded)\n",
        "\tx = keras.layers.Dropout(0.25)(x)\n",
        "\toutputs = keras.layers.Dense(6, activation=\"softmax\")(x)\n",
        "\n",
        "\tmodel = keras.Model(inputs, outputs, name=model_name)\n",
        "\n",
        "\tmodel.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "49a0JkYARGIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacf1e62-1f8a-4775-884f-2f1df9cd5496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"large_sequence_embedded_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 300)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 300, 256)          5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               73984     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,374\n",
            "Trainable params: 5,194,374\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "2196/2196 [==============================] - 353s 157ms/step - loss: 0.8941 - sparse_categorical_accuracy: 0.6127 - val_loss: 0.8263 - val_sparse_categorical_accuracy: 0.6400\n",
            "Epoch 2/30\n",
            "2196/2196 [==============================] - 318s 145ms/step - loss: 0.7890 - sparse_categorical_accuracy: 0.6594 - val_loss: 0.7912 - val_sparse_categorical_accuracy: 0.6565\n",
            "Epoch 3/30\n",
            "2196/2196 [==============================] - 318s 145ms/step - loss: 0.7515 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7842 - val_sparse_categorical_accuracy: 0.6590\n",
            "Epoch 4/30\n",
            "2196/2196 [==============================] - 319s 145ms/step - loss: 0.7248 - sparse_categorical_accuracy: 0.6904 - val_loss: 0.7975 - val_sparse_categorical_accuracy: 0.6540\n",
            "Epoch 5/30\n",
            "2196/2196 [==============================] - 320s 146ms/step - loss: 0.7023 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.7988 - val_sparse_categorical_accuracy: 0.6550\n",
            "Epoch 6/30\n",
            "2196/2196 [==============================] - 319s 145ms/step - loss: 0.6825 - sparse_categorical_accuracy: 0.7121 - val_loss: 0.8084 - val_sparse_categorical_accuracy: 0.6513\n",
            "Epoch 7/30\n",
            "2196/2196 [==============================] - 317s 144ms/step - loss: 0.6652 - sparse_categorical_accuracy: 0.7212 - val_loss: 0.8228 - val_sparse_categorical_accuracy: 0.6504\n",
            "Epoch 00007: early stopping\n",
            "Trained model in 37.0 minutes, 45 seconds\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.array_ops import zeros\n",
        "model_name = \"large_sequence_embedded_model\"\n",
        "\n",
        "model = create_embedding_model_categorical(max_tokens, model_name)\n",
        "\n",
        "# Create callback to save model with a given name\n",
        "model_path = f\"models/{model_name}.keras\"\n",
        "callbacks = [\n",
        "\tkeras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True),\n",
        "\tkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=False)\n",
        "]\n",
        "\n",
        "model.summary()\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Train Model\n",
        "model.fit(train_dataset_vectorized.cache(), \n",
        "\tvalidation_data=val_dataset_vectorized.cache(), \n",
        "\t# steps_per_epoch=10, \n",
        "\tepochs=30, \n",
        "\tcallbacks=callbacks\n",
        ")\n",
        "\n",
        "mins, secs = divmod(time.perf_counter() - start_time, 60)\n",
        "print(f\"Trained model in {mins} minutes, {round(secs)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model after training\n",
        "model = keras.models.load_model(model_path)\n",
        "predictions = model.predict(test_dataset_vectorized)\n",
        "predictions = np.argmax(predictions, axis = -1)\n",
        "true_labels = np.concatenate([y for _, y in test_dataset_vectorized], axis=0)\n",
        "mae = mean_absolute_error(true_labels, predictions)\n",
        "mse = mean_squared_error(true_labels, predictions)\n",
        "\n",
        "# Output Model Metrics\n",
        "metrics_text = f\"Model {model_name} with MAE {mae:.3f} and MSE {mse:.3f}\\n\"\n",
        "print(metrics_text)\n",
        "with open(\"model_metrics.txt\", \"a\") as f:\n",
        "\tf.write(metrics_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iOsDblPeLiH",
        "outputId": "9f272af1-2b4d-4669-afe0-3537fd1e6449"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model large_sequence_embedded_model with MAE 0.373 and MSE 0.472\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model with Text Vectorization layer\n",
        "inputs = keras.Input(shape=(None,), dtype=\"string\")\n",
        "vectorized_inputs = text_vectorization(inputs)\n",
        "outputs = model(vectorized_inputs)\n",
        "\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "\n",
        "keras.models.save_model(inference_model, \"models/full_text_model.tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6wtgpDq1IYy",
        "outputId": "6ba82b93-6fa2-446e-e1e5-1cf2a478b150"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: models/full_text_model.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: models/full_text_model.tf/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd2f86fc350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd3803e7e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "25MLzAoJRGIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "88c61c8f-873b-403b-9b8e-ba29fb0bbaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: models/ (stored 0%)\n",
            "  adding: models/large_sequence_embedded_model.keras (deflated 8%)\n",
            "  adding: models/full_text_model.tf/ (stored 0%)\n",
            "  adding: models/full_text_model.tf/keras_metadata.pb (deflated 91%)\n",
            "  adding: models/full_text_model.tf/variables/ (stored 0%)\n",
            "  adding: models/full_text_model.tf/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: models/full_text_model.tf/variables/variables.index (deflated 66%)\n",
            "  adding: models/full_text_model.tf/saved_model.pb (deflated 89%)\n",
            "  adding: models/full_text_model.tf/assets/ (stored 0%)\n",
            "  adding: models/text_vectorization_layer.tf/ (stored 0%)\n",
            "  adding: models/text_vectorization_layer.tf/keras_metadata.pb (deflated 81%)\n",
            "  adding: models/text_vectorization_layer.tf/variables/ (stored 0%)\n",
            "  adding: models/text_vectorization_layer.tf/variables/variables.data-00000-of-00001 (deflated 46%)\n",
            "  adding: models/text_vectorization_layer.tf/variables/variables.index (deflated 20%)\n",
            "  adding: models/text_vectorization_layer.tf/saved_model.pb (deflated 69%)\n",
            "  adding: models/text_vectorization_layer.tf/assets/ (stored 0%)\n",
            "  adding: models/full_text_model.keras (deflated 8%)\n",
            "  adding: models/.ipynb_checkpoints/ (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8701bf47-2e83-4c4d-8b8b-47b875593a75\", \"models.zip\", 96793946)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e5ea4247-5d50-4d0c-8b8a-98e344d5bbc0\", \"model_metrics.txt\", 65)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Zip Models\n",
        "!zip -r \"models.zip\" \"models\"\n",
        "\n",
        "try: \n",
        "\tfrom google.colab import files\n",
        "\tfiles.download(\"models.zip\")\n",
        "\tfiles.download(\"model_metrics.txt\")\n",
        "except:\n",
        "\tpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DrlWU6yrBXjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_large_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}